{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a (physics-based) model\n",
    "\n",
    "So far we've only looked at machine learning models. We are very keen to know\n",
    "how these \"smart\" approaches compare to more traditional, physics-based models.\n",
    "\n",
    "[PyPhenology](https://github.com/sdtaylor/pyPhenology) is a nice python package\n",
    "with a collection of physics-based models. We would like to compare those\n",
    "models, ideally within the same pycaret framework. However, pyPhenology is not\n",
    "consistent with the scikit-learn API. On the other hand, it is quite possible to\n",
    "cast their equations to a form that does adhere to these standards.\n",
    "\n",
    "In this notebook, we will walk you through the steps to create a custom\n",
    "estimator, following the [scikit-learn\n",
    "documentation](https://scikit-learn.org/stable/developers/develop.html). We will\n",
    "show how this is done for [pyPhenology's ThermalTime\n",
    "model](https://pyphenology.readthedocs.io/en/master/generated/pyPhenology.models.ThermalTime.html#pyPhenology.models.ThermalTime).\n",
    "At the end of the chapter, you should be able to repeat the trick for the other\n",
    "pyPhenology models as well.\n",
    "\n",
    "## The first blow is half the battle\n",
    "\n",
    "As a starting point, we copied the [scikit-learn project template](https://github.com/scikit-learn-contrib/project-template/blob/a06bc1a701fbb320848e4d5295e4477b596078df/skltemplate/_template.py) and updated it with some of the information from the [pyphenology ThermalTime](https://github.com/sdtaylor/pyPhenology/blob/d82af2f669364e84be4bf9325a4f4e064d8d3816/pyPhenology/models/thermaltime.py) class. Specifically, we:\n",
    "\n",
    "- Added\n",
    "  [RegressorMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html#sklearn.base.RegressorMixin)\n",
    "  from scikit-learn. This contains some methods specific to regression estimators.\n",
    "- Replaced `check_X_y` and `check_array` with the newer `_validate_data()` (see\n",
    "  [SLEP010](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html)).\n",
    "- Merged docstrings of ThermalTime and sklearn template\n",
    "- Changed to google-style docstrings and added type hints to method signatures instead of in the docstrings\n",
    "- Set the default values of the parameters to sensible ints instead of valid ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    RegressorMixin,\n",
    "    check_is_fitted,\n",
    ")\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "\n",
    "class ThermalTime(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"Thermal Time Model\n",
    "\n",
    "    The classic growing degree day model using a fixed temperature threshold\n",
    "    above which forcing accumulates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # TODO: consider adding DOY index series to fit/predict as optional argument\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike):\n",
    "        \"\"\"Fit the model to the available observations.\n",
    "\n",
    "        Parameters:\n",
    "            X: 2D Array of shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and\n",
    "                for each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            y: 1D Array of length n_samples\n",
    "                Observed DOY of the spring onset for each unique site/year.\n",
    "\n",
    "        Returns:\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        X, y = self._validate_data(X, y)\n",
    "        # TODO: check additional assumptions about input\n",
    "\n",
    "        # TODO: convert to proper fit; for now set some default values\n",
    "        self.t1_: int = 0\n",
    "        self.T_: int = 5\n",
    "        self.F_: int = 500\n",
    "\n",
    "        # `fit` should always return `self`\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: ArrayLike):\n",
    "        \"\"\"Predict values of y given new predictors\n",
    "\n",
    "        Parameters:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "               Daily mean temperatures for each unique site/year (n_samples) and\n",
    "               for each DOY (n_features). The first feature should correspond to\n",
    "               the first DOY, and so forth up to (max) 366.\n",
    "\n",
    "        Returns:\n",
    "            y: array-like, shape (n_samples,)\n",
    "               Predicted DOY of the spring onset for each sample in X.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X)\n",
    "        check_is_fitted(self, [\"t1_\", \"T_\", \"F_\"])\n",
    "\n",
    "        # TODO: Implement real predictions\n",
    "        return np.ones(X.shape[0], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! That's a big start! Notice that we're not passing in anything during initialization (yet). By convention of scikit-learn, the parameters of the model are only set during fit. Fitted parameters can be recognized by their trailing underscore.\n",
    "\n",
    "This class can already be used, although it doesn't\n",
    "actually fit or predict anything (useful) yet.\n",
    "\n",
    "However, before we proceed, let's see whether we already adhere to the\n",
    "scikit-learn API.\n",
    "\n",
    "## Checking sklearn compliance\n",
    "\n",
    "Scikit-learn provide a nice compliance checker. With a bit of extra code we can\n",
    "print out which tests it fails (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check check_regressors_train failed with exception: \n",
      "Check check_regressors_train failed with exception: \n",
      "Check check_regressors_train failed with exception: \n",
      "Passed checks: 37, failed checks: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "# This bit of code allows us to run the checks in a notebook\n",
    "checks = check_estimator(ThermalTime(), generate_only=True)\n",
    "passed_checks = 0\n",
    "failed_checks = 0\n",
    "for estimator, check in checks:\n",
    "    name = check.func.__name__\n",
    "    try:\n",
    "        check(estimator)\n",
    "        passed_checks += 1\n",
    "    except Exception as exc:\n",
    "        print(f\"Check {name} failed with exception: {exc}\")\n",
    "        failed_checks += 1\n",
    "print(f\"Passed checks: {passed_checks}, failed checks: {failed_checks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. Most of the checks passed, and if we dive deep into what's\n",
    "being check, we can figure out that the others failed because the predictions\n",
    "were not that good. That makes sense...\n",
    "\n",
    "We can inform the sklearn checker that this is expect by overriding the\n",
    "`poor_score` tag. Add the following method to your class:\n",
    "\n",
    "```python\n",
    "    def _more_tags(self):\n",
    "        # Pass checks related to performance of model as the thermaltime model\n",
    "        # cannot be expected to perform well for random data.\n",
    "        # https://scikit-learn.org/stable/developers/develop.html#estimator-tags\n",
    "        return {\n",
    "            'poor_score': True\n",
    "        }\n",
    "```\n",
    "\n",
    "Then, all checks should pass.\n",
    "\n",
    "## Using pytest and source files\n",
    "\n",
    "While it is possible to do all this in a notebook, a neater and more convenient\n",
    "way is to use pytest. To this end:\n",
    "\n",
    "- Store the class definition above in a new file called `thermaltime.py`\n",
    "- Create a new file called `test_thermaltime.py` and add the following content\n",
    "\n",
    "  ```py\n",
    "  from thermaltime import ThermalTime\n",
    "\n",
    "  from sklearn.utils.estimator_checks import parametrize_with_checks\n",
    "\n",
    "  @parametrize_with_checks([ThermalTime(),])\n",
    "  def test_sklearn_compatible_estimator(estimator, check):\n",
    "      check(estimator)\n",
    "  ```\n",
    "\n",
    "- Install pytest: `pip install pytest`\n",
    "- Run pytest: `pytest test_thermaltime.py`\n",
    "\n",
    "So far, so good! Next, we need to\n",
    "\n",
    "- Provide an implementation for the predict method\n",
    "- Provide an implementation for the fit method\n",
    "- Consider doing more validation of the input, since now we simply assume that\n",
    "  the data is in ordere columns from DOY 1 up to (max) 366.\n",
    "- Consider allowing an additional argument to fit and predict that contains the\n",
    "  column indices in case they're not neatly formatted from 1 to (max) 366. This\n",
    "  is allowed by scikit-learn as long as it's an optional argument.\n",
    "\n",
    "## Implementing predict\n",
    "\n",
    "We'll start by implementing the predict method. This is relatively\n",
    "straightforward. We'll write a simple function that takes both X and the\n",
    "parameters, and returns the expected DOY. For ease of reference, we copied the\n",
    "docstrings from above. This implementation should be exactly the same as in\n",
    "pyPhenology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy this function to your file thermaltime.py\n",
    "\n",
    "\n",
    "def predict_thermaltime(X, t1: int = 1, T: int = 5, F: int = 500):\n",
    "    \"\"\"Make prediction with the thermaltime model.\n",
    "\n",
    "    X: array-like, shape (n_samples, n_features).\n",
    "       Daily mean temperatures for each unique site/year (n_samples) and for\n",
    "       each DOY (n_features). The first feature should correspond to\n",
    "       the first DOY, and so forth up to (max) 366.\n",
    "    t1: The DOY at which forcing accumulating beings (should be within [-67,298])\n",
    "    T: The threshold above which forcing accumulates (should be within [-25,25])\n",
    "    F: The total forcing units required (should be within [0,1000])\n",
    "    \"\"\"\n",
    "    # This allows us to pass both 1D and 2D arrays of temperature\n",
    "    # Copying X to safely modify it later on (may not be necessary, but readable)\n",
    "    X_2d = np.atleast_2d(np.copy(X))\n",
    "\n",
    "    # DOY starts at 1, where python array index start at 0\n",
    "    # TODO: Make this an optional argument?\n",
    "    doy = np.arange(X_2d.shape[1]) + 1\n",
    "\n",
    "    # Exclude days before the start of the growing season\n",
    "    X_2d[:, doy < t1] = 0\n",
    "\n",
    "    # Exclude days with temperature below threshold\n",
    "    X_2d[X_2d < T] = 0\n",
    "\n",
    "    # Accumulate remaining data\n",
    "    S = np.cumsum(X_2d, axis=-1)\n",
    "\n",
    "    # Find first entry that exceeds the total forcing units required.\n",
    "    doy = np.argmax(S > F, axis=-1)\n",
    "\n",
    "    return doy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use this model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 degrees every day:\n",
    "X_test = np.ones(365) * 10\n",
    "\n",
    "# Predicted spring onset:\n",
    "predict_thermaltime(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 50, 50, 50, 50, 50, 50, 50, 50, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also check for 2D X inputs:\n",
    "X_test = np.ones((10, 365)) * 10\n",
    "predict_thermaltime(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it seems this works nicely, both for indivual prediction and for 2D arrays of inputs.\n",
    "\n",
    "### Adding tests\n",
    "\n",
    "These quick checks are super useful! We can quickly add a few more and add them to our test file (`test_thermaltime.py`). Note: also copy the `predict_thermaltime` function from above to your file `thermaltime.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy these tests to your file thermaltime.py, uncomment the imports, and remove the bottom part.\n",
    "\n",
    "# Note: these imports must be uncommented in your test file\n",
    "# import numpy as np\n",
    "# from thermaltime import ThermalTime, thermaltime\n",
    "\n",
    "\n",
    "def test_1d_base_case():\n",
    "    # 10 degrees every day:\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert predict_thermaltime(X_test) == 50\n",
    "\n",
    "\n",
    "def test_late_growing_season():\n",
    "    # If the growing season starts later, the spring onset is later as well.\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert predict_thermaltime(X_test, t1=11) == 60\n",
    "\n",
    "\n",
    "def test_higher_threshold():\n",
    "    # If the total accumulated forcing required is higher, spring onset is later.\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert predict_thermaltime(X_test, F=600) == 60\n",
    "\n",
    "\n",
    "def test_exclude_cold_days():\n",
    "    # If some days are below the minimum growing T, spring onset is later.\n",
    "    X_test = np.ones(365) * 10\n",
    "    X_test[[1, 4, 8, 12, 17, 24, 29, 33, 38, 42]] = 3\n",
    "    assert predict_thermaltime(X_test) == 60\n",
    "\n",
    "\n",
    "def test_lower_temperature_threshold():\n",
    "    # If the minimum growing T is lower, fewer days are exluded. However, the\n",
    "    # accumulated temperature rises more slowly.\n",
    "    X_test = np.ones(365) * 10\n",
    "\n",
    "    X_test[[1, 4, 8, 12, 17, 24, 29, 33, 38, 42]] = 5\n",
    "    assert predict_thermaltime(X_test, T=2) == 55\n",
    "\n",
    "\n",
    "def test_2d():\n",
    "    # Should be able to predict for multiple samples at once\n",
    "    X_test = np.ones((10, 365)) * 10\n",
    "    expected = np.ones(10) * 50\n",
    "    result = predict_thermaltime(X_test)\n",
    "    assert np.all(result == expected)\n",
    "\n",
    "\n",
    "# Note: The following lines are not needed in your test file. Pytest will\n",
    "# automatically call all functions starting with \"test_\".\n",
    "test_1d_base_case()\n",
    "test_late_growing_season()\n",
    "test_higher_threshold()\n",
    "test_exclude_cold_days()\n",
    "test_lower_temperature_threshold()\n",
    "test_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've copied the code to your files, you can run pytest again to check that all new tests pass.\n",
    "\n",
    "Now that we're confident our new predict function works, the last thing we need to do is update the predict method on the class. Change it to look like this:\n",
    "\n",
    "```py\n",
    "    def predict(self, X: ArrayLike):\n",
    "        \"\"\"Predict values of y given new predictors\n",
    "\n",
    "        Parameters:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "               Daily mean temperatures for each unique site/year (n_samples) and\n",
    "               for each DOY (n_features). The first feature should correspond to\n",
    "               the first DOY, and so forth up to (max) 366.\n",
    "\n",
    "        Returns:\n",
    "            y: array-like, shape (n_samples,)\n",
    "               Predicted DOY of the spring onset for each sample in X.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X)\n",
    "        check_is_fitted(self, [\"t1_\", \"T_\", \"F_\"])\n",
    "\n",
    "        return predict_thermaltime(X, self.t1_, self.T_, self.F_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the `fit` method\n",
    "\n",
    "Now that we can make predictions, we can also think about optimizing the parameters of the model.\n",
    "\n",
    "### Generate training data\n",
    "\n",
    "Our aim is to minimize the difference between the predictions based on the training data `X` and the target data `y`. Let's first prepare some training data to test the method once we have it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-25.002365102085015 50.045377945651\n",
      "44.55695961294945 57.3285262390998\n"
     ]
    }
   ],
   "source": [
    "# Prepare some training data. Take a base temperature of 10 degrees and add a\n",
    "# random fluctuation on top of it. The corresponding spring onset should\n",
    "# correlate with the random temperature fluctations.\n",
    "\n",
    "temp_signal = np.random.randn(10, 365)\n",
    "X_train = np.ones((10, 365)) * 10 + temp_signal * 10\n",
    "y_train = np.ones(10) * 50 + temp_signal.mean(axis=1) * 100\n",
    "\n",
    "# Check that the values are within somewhat realistic ranges\n",
    "print(X_train.min(), X_train.max())\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring algorithms\n",
    "\n",
    "Scipy has a lot of [optimization routines](https://docs.scipy.org/doc/scipy/reference/optimize.html). Looking at the list of optimizers, it seems `curve-fit` has the exact signature we are looking for. However, if we try it, we will quickly find out that it is not fit for purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.00000001,   5.        , 500.        ]),\n",
       " array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "initial_guess = [1, 5, 500]\n",
    "lower_bounds = [-67, -25, 0]\n",
    "upper_bounds = [298, 25, 1000]\n",
    "\n",
    "curve_fit(\n",
    "    predict_thermaltime,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    p0=initial_guess,\n",
    "    bounds=(lower_bounds, upper_bounds),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives terrible fits! Or rather, it doesn't seem to do much fitting at all. If you play around with the initial settings you'll see that most of the output is rubbish. That's probably because our prediction model is not a nice, continuous functional form. Also, `curve-fit` [doesn't really like](https://stackoverflow.com/a/22861933) integer parameters such as the DOY (though our implementation doesn't really care either).\n",
    "\n",
    "Clearly, we need to look further.\n",
    "\n",
    "### Reformulating the problem\n",
    "\n",
    "Alternatively, following pyphenology, we can try the [global optimizers](https://docs.scipy.org/doc/scipy/reference/optimize.html) instead. Looking at the documentation of the [brute-force optimizer](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brute.html#scipy.optimize.brute), it seems we need to reformulate our problem a little bit. First, we need to define a loss function, i.e. the function that scipy can minimize.\n",
    "\n",
    "Intuitively, our loss function could look something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.6190548414478645"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse_loss(X, y, t1: int = 1, T: int = 5, F: int = 500):\n",
    "    y_pred = predict_thermaltime(X, t1=t1, T=T, F=F)\n",
    "    sq_err = (y_pred - y) ** 2\n",
    "    return np.mean(sq_err) ** 0.5\n",
    "\n",
    "\n",
    "# Try it with the default parameters:\n",
    "rmse_loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not consistent with the form expected by scipy's global optimizers: they want something of the form `rmse_loss(x, *args)`. So we need to reformulate our loss function in terms of `x` and `args`.\n",
    "\n",
    "Nota bene: where the brute-force example finds the minimum of the function in the x-y space given fixed parameters, we want to do the opposite: we want to find the minimum in the parameter space, given the (fixed for each training batch) X and y. So, `x` is a tuple of our models params, `*args` corresponds to `X` and `y`, and our loss function becomes something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.6190548414478645"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse_loss(params, X, y):\n",
    "    \"\"\"RMSE loss between thermaltime predictions and observations.\n",
    "\n",
    "    params is a tuple with the parameters to thermaltime: (t1, T, F)\n",
    "    \"\"\"\n",
    "    y_pred = predict_thermaltime(X, *params)\n",
    "    sq_err = (y_pred - y) ** 2\n",
    "    return np.mean(sq_err) ** 0.5\n",
    "\n",
    "\n",
    "# Try it with the default parameters:\n",
    "rmse_loss([1, 5, 500], X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above is good enough, we can stil tweak our loss function a bit further while we're at it. Currently, our loss has the `predict_thermaltime` function hardcoded. It would be much nicer if this was more flexible, so it could work for any (roughly similar) model, for example, all the pyphenology models.\n",
    "\n",
    "One way to do this is to pass the predictor function as one of the arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.6190548414478645"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse_loss(params, f, X, y):\n",
    "    \"\"\"RMSE loss between thermaltime predictions and observations.\n",
    "\n",
    "    f is a prediction model of the form f(X, *params)\n",
    "    params is a tuple with the parameters to f\n",
    "    \"\"\"\n",
    "    y_pred = f(X, *params)\n",
    "    sq_err = (y_pred - y) ** 2\n",
    "    return np.mean(sq_err) ** 0.5\n",
    "\n",
    "\n",
    "# Try it with the default parameters:\n",
    "rmse_loss([1, 5, 500], predict_thermaltime, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force optimization\n",
    "\n",
    "Now we're ready to run the brute force optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48.26315789, 14.47368421,  0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import brute\n",
    "\n",
    "ranges = ((-67, 298), (-25, 25), (0, 1000))\n",
    "args = (predict_thermaltime, X_train, y_train)\n",
    "\n",
    "brute(rmse_loss, ranges=ranges, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works! If you look carefully at the results, you will see that the optimization algorithm finds a clever solution to the problem: it sets the total forcing required to 0, and the first day of the growing season very close to the typical spring onset. While this is a perfectly valid solution, especially considering the fake training data we used, it illustrates that you need to be really careful in setting the expected bounds of the parameters, and in interpreting the results.\n",
    "\n",
    "Another thing to note is how scipy creates the search space. The `brute` function has an additional parameter `Ns` that determines the number of 'grid points' used for each of the parameters. The default is 20: the ranges we supplied for `t1`, `T`, and `F` are divided in 20 points, such that the algorithm executes our loss function `20 * 20 * 20` times. By increasing the number of points, we get a better estimate, but the number of function calls and therefore the training time increases exponentially.\n",
    "\n",
    "There are two ways around this:\n",
    "\n",
    "- Set the points yourself\n",
    "- Use a more clever algorithm\n",
    "\n",
    "The first solution is quite simple. We can also define the ranges as slices with a custom `step`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43., -13., 105.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranges = (slice(-67, 298, 10), slice(-25, 25, 2), slice(0, 1000, 50))\n",
    "\n",
    "brute(rmse_loss, ranges=ranges, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basin hopping\n",
    "\n",
    "For the second solution, again, we follow the pyphenology developers. They support \"differential evolution\" and \"basin_hopping\". Let's try the basin hopping algorithm. This algorithm works in two steps, delegating calls to our loss function to a local minimization function. Thus, we need to pass in our bounds and args as `minimzer_kwargs` instead. We must also pick a method for the local minizer. For consistency with pyphenology, we choose 'LL-BFGS-B'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    message: ['requested number of basinhopping iterations completed successfully']\n",
       "                    success: True\n",
       "                        fun: 5.2379373393649695\n",
       "                          x: [ 3.063e+00  2.137e+00  5.034e+02]\n",
       "                        nit: 100\n",
       "      minimization_failures: 0\n",
       "                       nfev: 472\n",
       "                       njev: 118\n",
       " lowest_optimization_result:  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "                              success: True\n",
       "                               status: 0\n",
       "                                  fun: 5.2379373393649695\n",
       "                                    x: [ 3.063e+00  2.137e+00  5.034e+02]\n",
       "                                  nit: 0\n",
       "                                  jac: [ 0.000e+00  0.000e+00  0.000e+00]\n",
       "                                 nfev: 4\n",
       "                                 njev: 1\n",
       "                             hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import basinhopping\n",
    "\n",
    "initial_guess = (1, 5, 500)\n",
    "minimizer_args = {\n",
    "    \"method\": \"L-BFGS-B\",\n",
    "    \"args\": (predict_thermaltime, X_train, y_train),\n",
    "    \"bounds\": ((-67, 298), (-25, 25), (0, 1000)),\n",
    "}\n",
    "basinhopping(rmse_loss, x0=initial_guess, minimizer_kwargs=minimizer_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also works, but notice that the result is still very close to the initial\n",
    "guess. If we change the initial guess to something that's more like the clever\n",
    "solution of the brute method, we find another fit much smaller RMSE value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [48.16010104 13.83079594  0.77096103], minimum: 3.2652253589931624\n"
     ]
    }
   ],
   "source": [
    "initial_guess = (45, 15, 0)\n",
    "minimizer_args = {\n",
    "    \"method\": \"L-BFGS-B\",\n",
    "    \"args\": (predict_thermaltime, X_train, y_train),\n",
    "    \"bounds\": ((-67, 298), (-25, 25), (0, 1000)),\n",
    "}\n",
    "\n",
    "bh = basinhopping(rmse_loss, x0=initial_guess, minimizer_kwargs=minimizer_args)\n",
    "print(f\"params: {bh.x}, minimum: {bh.fun}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can pass additional arguments to the basinhopping algorithm to tweak its performance. For example, the default of 100 iterations is quite small. Using the settings from pyphenology, we get a result that looks much more realistic (and takes much longer to train):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    message: ['requested number of basinhopping iterations completed successfully']\n",
       "                    success: True\n",
       "                        fun: 2.893246475688441\n",
       "                          x: [ 4.712e+01  1.243e+01  3.338e+01]\n",
       "                        nit: 50000\n",
       "      minimization_failures: 0\n",
       "                       nfev: 200072\n",
       "                       njev: 50018\n",
       " lowest_optimization_result:  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "                              success: True\n",
       "                               status: 0\n",
       "                                  fun: 2.893246475688441\n",
       "                                    x: [ 4.712e+01  1.243e+01  3.338e+01]\n",
       "                                  nit: 0\n",
       "                                  jac: [ 0.000e+00  0.000e+00  0.000e+00]\n",
       "                                 nfev: 4\n",
       "                                 njev: 1\n",
       "                             hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import basinhopping\n",
    "\n",
    "initial_guess = (1, 5, 500)\n",
    "minimizer_args = {\n",
    "    \"method\": \"L-BFGS-B\",\n",
    "    \"args\": (predict_thermaltime, X_train, y_train),\n",
    "    \"bounds\": ((-67, 298), (-25, 25), (0, 1000)),\n",
    "}\n",
    "optimizer_params = {\"niter\": 50000, \"T\": 0.5, \"stepsize\": 0.5, \"disp\": False}\n",
    "basinhopping(\n",
    "    rmse_loss, x0=initial_guess, **optimizer_params, minimizer_kwargs=minimizer_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a good grip on the different ways to fit our model. A final thing to point out is that the basinhopping algorithm returns floats for each parameter. This doesn't really make sense for the start of the growing season, so in the post-processing we should probably round that to the nearest int.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing everything together\n",
    "\n",
    "If we bring everything from above over to the `fit` method, we get something like this:\n",
    "\n",
    "```python\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike):\n",
    "        \"\"\"Fit the model to the available observations.\n",
    "\n",
    "        Parameters:\n",
    "            X: 2D Array of shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and\n",
    "                for each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            y: 1D Array of length n_samples\n",
    "                Observed DOY of the spring onset for each unique site/year.\n",
    "\n",
    "        Returns:\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        X, y = self._validate_data(X, y)\n",
    "        # TODO: check additional assumptions about input\n",
    "\n",
    "        initial_guess = (1, 5, 500)\n",
    "        bounds = ((-67, 298), (-25, 25), (0, 1000))\n",
    "\n",
    "        minimizer_args = {\n",
    "            \"method\": \"L-BFGS-B\",\n",
    "            \"args\": (predict_thermaltime, X, y),\n",
    "            \"bounds\": bounds,\n",
    "        }\n",
    "        optimizer_params = {\"niter\": 50000, \"T\": 0.5, \"stepsize\": 0.5, \"disp\": False}\n",
    "\n",
    "        bh = basinhopping(\n",
    "            rmse_loss,\n",
    "            x0=initial_guess,\n",
    "            minimizer_kwargs=minimizer_args,\n",
    "            **optimizer_params,\n",
    "        )\n",
    "\n",
    "        self.t1_, self.T_, self.F_ = bh.x\n",
    "\n",
    "        return self\n",
    "```\n",
    "\n",
    "This is not yet ready:\n",
    "\n",
    "- `Initial guess` and `bounds` are hardcoded and closely related to the equally\n",
    "  hardcoded `thermaltime` and the loss function. If we want to reuse this for\n",
    "  other physics-based models, it would be nice to try and separate these a bit\n",
    "  better.\n",
    "- The `optimizer_params` are hardcoded as well. These are typical (hyper)\n",
    "  parameters that can be passed to the `__init__` method of our class, since\n",
    "  these settings are not data-dependent.\n",
    "- Currently the only optimization routine supported is the basinhopping\n",
    "  algorithm.\n",
    "\n",
    "### Putting it to the test\n",
    "\n",
    "Below we pasted a slighly more polished version, as a complete code. We\n",
    "generalized the loss function a bit better, and the core_model-related\n",
    "parameters are grouped together at the top. The fit method is mostly concerned\n",
    "with formulating the call to the basinhopping algorithm. We used the\n",
    "`staticmethod` decorator to make the thermaltime function a method on the class.\n",
    "Notice that, in contrast with standard class methods, it doesn't need access to\n",
    "`self`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    RegressorMixin,\n",
    "    check_is_fitted,\n",
    ")\n",
    "from numpy.typing import ArrayLike\n",
    "from scipy.optimize import basinhopping\n",
    "\n",
    "\n",
    "def rmse_loss(params, f, X, y):\n",
    "    \"\"\"RMSE loss between thermaltime predictions and observations.\n",
    "\n",
    "    f is a prediction model of the form f(X, *params)\n",
    "    params is a tuple with the parameters to f\n",
    "    \"\"\"\n",
    "    y_pred = f(X, *params)\n",
    "    sq_err = (y_pred - y) ** 2\n",
    "    return np.mean(sq_err) ** 0.5\n",
    "\n",
    "\n",
    "LOSS_FUNCTIONS = {\n",
    "    \"RMSE\": rmse_loss,\n",
    "}\n",
    "\n",
    "\n",
    "class ThermalTime(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"Thermal Time Model\n",
    "\n",
    "    The classic growing degree day model using a fixed temperature threshold\n",
    "    above which forcing accumulates.\n",
    "\n",
    "    This implementation uses scipy's basinhopper optimization algorithm for\n",
    "    fitting the model.\n",
    "    \"\"\"\n",
    "\n",
    "    _core_params_names = (\"t1\", \"T\", \"F\")\n",
    "    _core_params_defaults = (1, 5, 500)\n",
    "    _core_params_bounds = ((-67, 298), (-25, 25), (0, 1000))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        niter=50000,\n",
    "        T=0.5,\n",
    "        stepsize=0.5,\n",
    "        disp=False,\n",
    "        minimizer_method=\"L-BFGS-B\",\n",
    "        loss_function=\"RMSE\",\n",
    "    ):\n",
    "        self.niter = niter\n",
    "        self.T = T\n",
    "        self.stepsize = stepsize\n",
    "        self.disp = disp\n",
    "        self.minimizer_method = minimizer_method\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    @staticmethod\n",
    "    def _core_model(X, t1: int = 1, T: int = 5, F: int = 500):\n",
    "        \"\"\"Make prediction with the thermaltime model.\n",
    "\n",
    "        Args:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and for\n",
    "                each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            t1: The DOY at which forcing accumulating beings (should be within [-67,298])\n",
    "            T: The threshold above which forcing accumulates (should be within [-25,25])\n",
    "            F: The total forcing units required (should be within [0,1000])\n",
    "        \"\"\"\n",
    "        # This allows us to pass both 1D and 2D arrays of temperature\n",
    "        # Copying X to safely modify it later on (may not be necessary, but readable)\n",
    "        X_2d = np.atleast_2d(np.copy(X))\n",
    "\n",
    "        # DOY starts at 1, where python array index start at 0\n",
    "        # TODO: Make this an optional argument?\n",
    "        doy = np.arange(X_2d.shape[1]) + 1\n",
    "\n",
    "        # Exclude days before the start of the growing season\n",
    "        X_2d[:, doy < t1] = 0\n",
    "\n",
    "        # Exclude days with temperature below threshold\n",
    "        X_2d[X_2d < T] = 0\n",
    "\n",
    "        # Accumulate remaining data\n",
    "        S = np.cumsum(X_2d, axis=-1)\n",
    "\n",
    "        # Find first entry that exceeds the total forcing units required.\n",
    "        doy = np.argmax(S > F, axis=-1)\n",
    "\n",
    "        return doy\n",
    "\n",
    "    # TODO: consider adding DOY index series to fit/predict as optional argument\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike):\n",
    "        \"\"\"Fit the model to the available observations.\n",
    "\n",
    "        Parameters:\n",
    "            X: 2D Array of shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and\n",
    "                for each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            y: 1D Array of length n_samples\n",
    "                Observed DOY of the spring onset for each unique site/year.\n",
    "\n",
    "        Returns:\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        X, y = self._validate_data(X, y)\n",
    "        # TODO: check additional assumptions about input\n",
    "\n",
    "        loss_function = LOSS_FUNCTIONS[self.loss_function]\n",
    "\n",
    "        # Perform the fit\n",
    "        bh = basinhopping(\n",
    "            loss_function,\n",
    "            x0=self._core_params_defaults,\n",
    "            niter=self.niter,\n",
    "            T=self.T,\n",
    "            stepsize=self.stepsize,\n",
    "            disp=self.disp,\n",
    "            minimizer_kwargs={\n",
    "                \"method\": self.minimizer_method,\n",
    "                \"args\": (self._core_model, X, y),\n",
    "                \"bounds\": self._core_params_bounds,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Store the fitted parameters\n",
    "        self.core_params_ = bh.x\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: ArrayLike):\n",
    "        \"\"\"Predict values of y given new predictors\n",
    "\n",
    "        Parameters:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "               Daily mean temperatures for each unique site/year (n_samples) and\n",
    "               for each DOY (n_features). The first feature should correspond to\n",
    "               the first DOY, and so forth up to (max) 366.\n",
    "\n",
    "        Returns:\n",
    "            y: array-like, shape (n_samples,)\n",
    "               Predicted DOY of the spring onset for each sample in X.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X)\n",
    "        check_is_fitted(self, \"core_params_\")\n",
    "\n",
    "        return self._core_model(X, *self.core_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While still not perfect, this implementation is good enough to see if we can use\n",
    "it in our pycaret framework. For that, we first need to combine our data in a\n",
    "pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.218994</td>\n",
       "      <td>6.738299</td>\n",
       "      <td>13.954624</td>\n",
       "      <td>29.450302</td>\n",
       "      <td>-3.373456</td>\n",
       "      <td>17.349404</td>\n",
       "      <td>11.489199</td>\n",
       "      <td>-0.946024</td>\n",
       "      <td>9.447005</td>\n",
       "      <td>14.721903</td>\n",
       "      <td>...</td>\n",
       "      <td>17.564457</td>\n",
       "      <td>8.209593</td>\n",
       "      <td>10.716687</td>\n",
       "      <td>2.374732</td>\n",
       "      <td>22.605012</td>\n",
       "      <td>2.639938</td>\n",
       "      <td>25.742186</td>\n",
       "      <td>9.589601</td>\n",
       "      <td>-4.323429</td>\n",
       "      <td>48.583246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.773274</td>\n",
       "      <td>19.317239</td>\n",
       "      <td>-0.591967</td>\n",
       "      <td>13.633791</td>\n",
       "      <td>4.640599</td>\n",
       "      <td>2.588708</td>\n",
       "      <td>11.031384</td>\n",
       "      <td>10.298080</td>\n",
       "      <td>-1.175578</td>\n",
       "      <td>-20.222365</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.453457</td>\n",
       "      <td>7.593247</td>\n",
       "      <td>4.730469</td>\n",
       "      <td>11.483520</td>\n",
       "      <td>14.818011</td>\n",
       "      <td>25.799254</td>\n",
       "      <td>15.006352</td>\n",
       "      <td>3.442988</td>\n",
       "      <td>-3.945578</td>\n",
       "      <td>50.461037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.166114</td>\n",
       "      <td>6.776026</td>\n",
       "      <td>3.693059</td>\n",
       "      <td>8.057823</td>\n",
       "      <td>5.018413</td>\n",
       "      <td>23.824257</td>\n",
       "      <td>0.922434</td>\n",
       "      <td>-1.134289</td>\n",
       "      <td>17.514239</td>\n",
       "      <td>10.534730</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.523276</td>\n",
       "      <td>15.888692</td>\n",
       "      <td>-3.116856</td>\n",
       "      <td>3.443013</td>\n",
       "      <td>4.458324</td>\n",
       "      <td>12.345812</td>\n",
       "      <td>8.165304</td>\n",
       "      <td>21.404692</td>\n",
       "      <td>5.190300</td>\n",
       "      <td>52.001860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.454030</td>\n",
       "      <td>12.983264</td>\n",
       "      <td>18.690565</td>\n",
       "      <td>14.266377</td>\n",
       "      <td>13.291256</td>\n",
       "      <td>9.783686</td>\n",
       "      <td>18.153094</td>\n",
       "      <td>15.724669</td>\n",
       "      <td>-13.733181</td>\n",
       "      <td>7.353611</td>\n",
       "      <td>...</td>\n",
       "      <td>11.454641</td>\n",
       "      <td>11.915009</td>\n",
       "      <td>10.903428</td>\n",
       "      <td>8.284341</td>\n",
       "      <td>0.811677</td>\n",
       "      <td>9.052229</td>\n",
       "      <td>7.078002</td>\n",
       "      <td>0.597021</td>\n",
       "      <td>10.312079</td>\n",
       "      <td>57.328526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.336732</td>\n",
       "      <td>5.656002</td>\n",
       "      <td>9.542669</td>\n",
       "      <td>15.932466</td>\n",
       "      <td>22.050319</td>\n",
       "      <td>5.539453</td>\n",
       "      <td>10.183360</td>\n",
       "      <td>4.398088</td>\n",
       "      <td>4.344155</td>\n",
       "      <td>10.000675</td>\n",
       "      <td>...</td>\n",
       "      <td>6.672839</td>\n",
       "      <td>29.814395</td>\n",
       "      <td>8.529970</td>\n",
       "      <td>1.693037</td>\n",
       "      <td>1.629311</td>\n",
       "      <td>0.112947</td>\n",
       "      <td>12.897052</td>\n",
       "      <td>24.386493</td>\n",
       "      <td>3.704314</td>\n",
       "      <td>44.556960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 366 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0  15.218994   6.738299  13.954624  29.450302  -3.373456  17.349404   \n",
       "1  22.773274  19.317239  -0.591967  13.633791   4.640599   2.588708   \n",
       "2  16.166114   6.776026   3.693059   8.057823   5.018413  23.824257   \n",
       "3  -4.454030  12.983264  18.690565  14.266377  13.291256   9.783686   \n",
       "4  11.336732   5.656002   9.542669  15.932466  22.050319   5.539453   \n",
       "\n",
       "           6          7          8          9  ...        356        357  \\\n",
       "0  11.489199  -0.946024   9.447005  14.721903  ...  17.564457   8.209593   \n",
       "1  11.031384  10.298080  -1.175578 -20.222365  ...  -1.453457   7.593247   \n",
       "2   0.922434  -1.134289  17.514239  10.534730  ...  -1.523276  15.888692   \n",
       "3  18.153094  15.724669 -13.733181   7.353611  ...  11.454641  11.915009   \n",
       "4  10.183360   4.398088   4.344155  10.000675  ...   6.672839  29.814395   \n",
       "\n",
       "         358        359        360        361        362        363  \\\n",
       "0  10.716687   2.374732  22.605012   2.639938  25.742186   9.589601   \n",
       "1   4.730469  11.483520  14.818011  25.799254  15.006352   3.442988   \n",
       "2  -3.116856   3.443013   4.458324  12.345812   8.165304  21.404692   \n",
       "3  10.903428   8.284341   0.811677   9.052229   7.078002   0.597021   \n",
       "4   8.529970   1.693037   1.629311   0.112947  12.897052  24.386493   \n",
       "\n",
       "         364          y  \n",
       "0  -4.323429  48.583246  \n",
       "1  -3.945578  50.461037  \n",
       "2   5.190300  52.001860  \n",
       "3  10.312079  57.328526  \n",
       "4   3.704314  44.556960  \n",
       "\n",
       "[5 rows x 366 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_train)\n",
    "df[\"y\"] = y_train\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_86850_row8_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_86850\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_86850_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_86850_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_86850_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_86850_row0_col1\" class=\"data row0 col1\" >8334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_86850_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_86850_row1_col1\" class=\"data row1 col1\" >y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_86850_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_86850_row2_col1\" class=\"data row2 col1\" >Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_86850_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_86850_row3_col1\" class=\"data row3 col1\" >(10, 366)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_86850_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_86850_row4_col1\" class=\"data row4 col1\" >(10, 366)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_86850_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_86850_row5_col1\" class=\"data row5 col1\" >(7, 366)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_86850_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_86850_row6_col1\" class=\"data row6 col1\" >(3, 366)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_86850_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_86850_row7_col1\" class=\"data row7 col1\" >365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_86850_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n",
       "      <td id=\"T_86850_row8_col1\" class=\"data row8 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_86850_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n",
       "      <td id=\"T_86850_row9_col1\" class=\"data row9 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_86850_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_86850_row10_col1\" class=\"data row10 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_86850_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_86850_row11_col1\" class=\"data row11 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_86850_row12_col0\" class=\"data row12 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_86850_row12_col1\" class=\"data row12 col1\" >KFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_86850_row13_col0\" class=\"data row13 col0\" >Fold Number</td>\n",
       "      <td id=\"T_86850_row13_col1\" class=\"data row13 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_86850_row14_col0\" class=\"data row14 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_86850_row14_col1\" class=\"data row14 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_86850_row15_col0\" class=\"data row15 col0\" >Use GPU</td>\n",
       "      <td id=\"T_86850_row15_col1\" class=\"data row15 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_86850_row16_col0\" class=\"data row16 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_86850_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_86850_row17_col0\" class=\"data row17 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_86850_row17_col1\" class=\"data row17 col1\" >reg-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86850_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_86850_row18_col0\" class=\"data row18 col0\" >USI</td>\n",
       "      <td id=\"T_86850_row18_col1\" class=\"data row18 col1\" >c9d5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f52aa84a860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_accdc th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_accdc_row0_col0, #T_accdc_row1_col0, #T_accdc_row1_col1, #T_accdc_row1_col2, #T_accdc_row1_col3, #T_accdc_row1_col4, #T_accdc_row1_col5, #T_accdc_row1_col6, #T_accdc_row2_col0, #T_accdc_row2_col1, #T_accdc_row2_col2, #T_accdc_row2_col3, #T_accdc_row2_col4, #T_accdc_row2_col5, #T_accdc_row2_col6, #T_accdc_row3_col0, #T_accdc_row3_col1, #T_accdc_row3_col2, #T_accdc_row3_col3, #T_accdc_row3_col4, #T_accdc_row3_col5, #T_accdc_row3_col6 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_accdc_row0_col1, #T_accdc_row0_col2, #T_accdc_row0_col3, #T_accdc_row0_col4, #T_accdc_row0_col5, #T_accdc_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_accdc_row0_col7, #T_accdc_row1_col7, #T_accdc_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_accdc_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_accdc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_accdc_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_accdc_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_accdc_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_accdc_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_accdc_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_accdc_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_accdc_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "      <th id=\"T_accdc_level0_col7\" class=\"col_heading level0 col7\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_accdc_level0_row0\" class=\"row_heading level0 row0\" >3</th>\n",
       "      <td id=\"T_accdc_row0_col0\" class=\"data row0 col0\" >ThermalTime</td>\n",
       "      <td id=\"T_accdc_row0_col1\" class=\"data row0 col1\" >3.7702</td>\n",
       "      <td id=\"T_accdc_row0_col2\" class=\"data row0 col2\" >20.0271</td>\n",
       "      <td id=\"T_accdc_row0_col3\" class=\"data row0 col3\" >4.4752</td>\n",
       "      <td id=\"T_accdc_row0_col4\" class=\"data row0 col4\" >0.2647</td>\n",
       "      <td id=\"T_accdc_row0_col5\" class=\"data row0 col5\" >0.0856</td>\n",
       "      <td id=\"T_accdc_row0_col6\" class=\"data row0 col6\" >0.0736</td>\n",
       "      <td id=\"T_accdc_row0_col7\" class=\"data row0 col7\" >64.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_accdc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_accdc_row1_col0\" class=\"data row1 col0\" >Random Forest Regressor</td>\n",
       "      <td id=\"T_accdc_row1_col1\" class=\"data row1 col1\" >4.1401</td>\n",
       "      <td id=\"T_accdc_row1_col2\" class=\"data row1 col2\" >25.6182</td>\n",
       "      <td id=\"T_accdc_row1_col3\" class=\"data row1 col3\" >5.0614</td>\n",
       "      <td id=\"T_accdc_row1_col4\" class=\"data row1 col4\" >0.0594</td>\n",
       "      <td id=\"T_accdc_row1_col5\" class=\"data row1 col5\" >0.0977</td>\n",
       "      <td id=\"T_accdc_row1_col6\" class=\"data row1 col6\" >0.0818</td>\n",
       "      <td id=\"T_accdc_row1_col7\" class=\"data row1 col7\" >0.6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_accdc_level0_row2\" class=\"row_heading level0 row2\" >0</th>\n",
       "      <td id=\"T_accdc_row2_col0\" class=\"data row2 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_accdc_row2_col1\" class=\"data row2 col1\" >4.2148</td>\n",
       "      <td id=\"T_accdc_row2_col2\" class=\"data row2 col2\" >25.9996</td>\n",
       "      <td id=\"T_accdc_row2_col3\" class=\"data row2 col3\" >5.0990</td>\n",
       "      <td id=\"T_accdc_row2_col4\" class=\"data row2 col4\" >0.0454</td>\n",
       "      <td id=\"T_accdc_row2_col5\" class=\"data row2 col5\" >0.0977</td>\n",
       "      <td id=\"T_accdc_row2_col6\" class=\"data row2 col6\" >0.0823</td>\n",
       "      <td id=\"T_accdc_row2_col7\" class=\"data row2 col7\" >0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_accdc_level0_row3\" class=\"row_heading level0 row3\" >2</th>\n",
       "      <td id=\"T_accdc_row3_col0\" class=\"data row3 col0\" >Dummy Regressor</td>\n",
       "      <td id=\"T_accdc_row3_col1\" class=\"data row3 col1\" >4.4073</td>\n",
       "      <td id=\"T_accdc_row3_col2\" class=\"data row3 col2\" >27.8321</td>\n",
       "      <td id=\"T_accdc_row3_col3\" class=\"data row3 col3\" >5.2756</td>\n",
       "      <td id=\"T_accdc_row3_col4\" class=\"data row3 col4\" >-0.0218</td>\n",
       "      <td id=\"T_accdc_row3_col5\" class=\"data row3 col5\" >0.1014</td>\n",
       "      <td id=\"T_accdc_row3_col6\" class=\"data row3 col6\" >0.0863</td>\n",
       "      <td id=\"T_accdc_row3_col7\" class=\"data row3 col7\" >0.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f52ab1b0220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ThermalTime()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ThermalTime</label><div class=\"sk-toggleable__content\"><pre>ThermalTime()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ThermalTime()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pycaret.regression import RegressionExperiment\n",
    "\n",
    "exp = RegressionExperiment()\n",
    "exp.setup(data=df, target=\"y\")\n",
    "exp.compare_models([\"lr\", \"rf\", \"dummy\", ThermalTime()], cross_validation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, but not surprisingly, our new model is not very skillful. If you\n",
    "execute this notebook several times, scores will vary from pretty okay to very\n",
    "bad. But this is to be expected of a training set of 10 samples based on some\n",
    "vaguely invented data...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards a generic implementation\n",
    "\n",
    "We hope this tutorial has provided a good basis for developing this further.\n",
    "There's still a number of things that could be improved further. For example, we\n",
    "mentioned the option to pass in additional arguments to the `fit` method based,\n",
    "e.g. to use a deviating DOY index. If you look at the code in\n",
    "[springtime.models](https://github.com/phenology/springtime/tree/eae0caa69d3d20836407ebf5f939cde6ba325827/src/springtime/models),\n",
    "you will see that we have generalized the code a bit further. To add a new\n",
    "model, as long as it's similar enough to the thermaltime model, you only need to\n",
    "implement the analogue of the `thermaltime_predict` method.\n",
    "\n",
    "Compared to pyphenology, an important difference is that we based our main class\n",
    "off the correpsonding scipy estimator, and you can pass in different phenology\n",
    "models. By contrast, in pyphenology, there is one main class for each phenology\n",
    "model, while you can specify the optimization algorithm. There is something to\n",
    "say for both choices; we feel the present implementation aligns a bit better\n",
    "with the way in which sklearn defines hyperparameters of a model. Thus, you can\n",
    "now use this model in springtime by importing it like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BasinHopper()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BasinHopper</label><div class=\"sk-toggleable__content\"><pre>BasinHopper()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BasinHopper()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from springtime.models.basinhopper import BasinHopper\n",
    "\n",
    "model = BasinHopper(core_model=\"thermaltime\", loss_function=\"RMSE\")\n",
    "model.fit(X_train, y_train)\n",
    "# model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "springtime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
