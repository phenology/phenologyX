{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a (physics-based) model\n",
    "\n",
    "So far we've only looked at machine learning models. We are very keen to know\n",
    "how these \"smart\" approaches compare to more traditional, physics-based models.\n",
    "\n",
    "[PyPhenology](https://github.com/sdtaylor/pyPhenology) is a nice python package\n",
    "with a collection of physics-based models. We would like to compare those\n",
    "models, ideally within the same pycaret framework. However, pyPhenology is not\n",
    "consistent with the scikit-learn API. On the other hand, it is quite possible to\n",
    "cast their equations to a form that does adhere to these standards.\n",
    "\n",
    "In this notebook, we will walk you through the steps to create a custom\n",
    "estimator, following the [scikit-learn\n",
    "documentation](https://scikit-learn.org/stable/developers/develop.html). We will\n",
    "show how this is done for [pyPhenology's ThermalTime\n",
    "model](https://pyphenology.readthedocs.io/en/master/generated/pyPhenology.models.ThermalTime.html#pyPhenology.models.ThermalTime).\n",
    "At the end of the chapter, you should be able to repeat the trick for the other\n",
    "pyPhenology models as well.\n",
    "\n",
    "## The first blow is half the battle\n",
    "\n",
    "As a starting point, we copied the [scikit-learn project template](https://github.com/scikit-learn-contrib/project-template/blob/a06bc1a701fbb320848e4d5295e4477b596078df/skltemplate/_template.py) and updated it with some of the information from the [pyphenology ThermalTime](https://github.com/sdtaylor/pyPhenology/blob/d82af2f669364e84be4bf9325a4f4e064d8d3816/pyPhenology/models/thermaltime.py) class. Specifically, we:\n",
    "\n",
    "- Added\n",
    "  [RegressorMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html#sklearn.base.RegressorMixin)\n",
    "  from scikit-learn. This contains some methods specific to regression estimators.\n",
    "- Replaced `check_X_y` and `check_array` with the newer `_validate_data()` (see\n",
    "  [SLEP010](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html)).\n",
    "- Merged docstrings of ThermalTime and sklearn template\n",
    "- Changed to google-style docstrings and added type hints to method signatures instead of in the docstrings\n",
    "- Set the default values of the parameters to sensible ints instead of valid ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    RegressorMixin,\n",
    "    check_is_fitted,\n",
    ")\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "\n",
    "class ThermalTime(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"Thermal Time Model\n",
    "\n",
    "    The classic growing degree day model using a fixed temperature threshold\n",
    "    above which forcing accumulates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # TODO: consider adding DOY index series to fit/predict as optional argument\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike):\n",
    "        \"\"\"Fit the model to the available observations.\n",
    "\n",
    "        Parameters:\n",
    "            X: 2D Array of shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and\n",
    "                for each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            y: 1D Array of length n_samples\n",
    "                Observed DOY of the spring onset for each unique site/year.\n",
    "\n",
    "        Returns:\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        X, y = self._validate_data(X, y)\n",
    "        # TODO: check additional assumptions about input\n",
    "\n",
    "        # TODO: convert to proper fit; for now set some default values\n",
    "        self.t1_: int = 0\n",
    "        self.T_: int = 5\n",
    "        self.F_: int = 500\n",
    "\n",
    "        # `fit` should always return `self`\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: ArrayLike):\n",
    "        \"\"\"Predict values of y given new predictors\n",
    "\n",
    "        Parameters:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "               Daily mean temperatures for each unique site/year (n_samples) and\n",
    "               for each DOY (n_features). The first feature should correspond to\n",
    "               the first DOY, and so forth up to (max) 366.\n",
    "\n",
    "        Returns:\n",
    "            y: array-like, shape (n_samples,)\n",
    "               Predicted DOY of the spring onset for each sample in X.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X)\n",
    "        check_is_fitted(self, [\"t1_\", \"T_\", \"F_\"])\n",
    "\n",
    "        # TODO: Implement real predictions\n",
    "        return np.ones(X.shape[0], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! That's a big start! Notice that we're not passing in anything during initialization (yet). By convention of scikit-learn, the parameters of the model are only set during fit. Fitted parameters can be recognized by their trailing underscore.\n",
    "\n",
    "This class can already be used, although it doesn't\n",
    "actually fit or predict anything (useful) yet. Next, we need to\n",
    "\n",
    "- Provide an implementation for the predict method\n",
    "- Provide an implementation for the fit method\n",
    "- Consider doing more validation of the input, since now we simply assume that\n",
    "  the data is in ordere columns from DOY 1 up to (max) 366.\n",
    "- Consider allowing an additional argument to fit and predict that contains the\n",
    "  column indices in case they're not neatly formatted from 1 to (max) 366. This\n",
    "  is allowed by scikit-learn as long as it's an optional argument.\n",
    "\n",
    "However, before we proceed, let's see whether we already adhere to the\n",
    "scikit-learn API.\n",
    "\n",
    "## Checking sklearn compliance\n",
    "\n",
    "Scikit-learn provide a nice compliance checker. With a bit of extra code we can\n",
    "print out which tests it fails (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check check_regressors_train failed with exception: \n",
      "Check check_regressors_train failed with exception: \n",
      "Check check_regressors_train failed with exception: \n",
      "Passed checks: 37, failed checks: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "# This bit of code allows us to run the checks in a notebook\n",
    "checks = check_estimator(ThermalTime(), generate_only=True)\n",
    "passed_checks = 0\n",
    "failed_checks = 0\n",
    "for estimator, check in checks:\n",
    "    name = check.func.__name__\n",
    "    try:\n",
    "        check(estimator)\n",
    "        passed_checks += 1\n",
    "    except Exception as exc:\n",
    "        print(f\"Check {name} failed with exception: {exc}\")\n",
    "        failed_checks += 1\n",
    "print(f\"Passed checks: {passed_checks}, failed checks: {failed_checks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. Most of the checks passed, and if we dive deep into what's\n",
    "being check, we can figure out that the others failed because the predictions\n",
    "were not that good. That makes sense...\n",
    "\n",
    "## Using pytest and source files\n",
    "\n",
    "While it is possible to do all this in a notebook, a neater and more convenient\n",
    "way is to use pytest. To this end:\n",
    "\n",
    "- Store the class definition above in a new file called `thermaltime.py`\n",
    "- Create a new file called `test_thermaltime.py` and add the following content\n",
    "\n",
    "  ```py\n",
    "  from thermaltime import ThermalTime\n",
    "\n",
    "  from sklearn.utils.estimator_checks import parametrize_with_checks\n",
    "\n",
    "  @parametrize_with_checks([ThermalTime(),])\n",
    "  def test_sklearn_compatible_estimator(estimator, check):\n",
    "      check(estimator)\n",
    "  ```\n",
    "\n",
    "- Install pytest: `pip install pytest`\n",
    "- Run pytest: `pytest test_thermaltime.py`\n",
    "\n",
    "## Implementing predict\n",
    "\n",
    "We'll start by implementing the predict method. This is relatively\n",
    "straightforward. We'll write a simple function that takes both X and the\n",
    "parameters, and returns the expected DOY. For ease of reference, we copied the\n",
    "docstrings from above. This implementation should be exactly the same as in\n",
    "pyPhenology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy this function to your file thermaltime.py\n",
    "\n",
    "\n",
    "def thermaltime(X, t1: int = 0, T: int = 5, F: int = 500):\n",
    "    \"\"\"Make prediction with the thermaltime model.\n",
    "\n",
    "    X: array-like, shape (n_samples, n_features).\n",
    "       Daily mean temperatures for each unique site/year (n_samples) and for\n",
    "       each DOY (n_features). The first feature should correspond to\n",
    "       the first DOY, and so forth up to (max) 366.\n",
    "    t1: The DOY at which forcing accumulating beings (should be within [-67,298])\n",
    "    T: The threshold above which forcing accumulates (should be within [-25,25])\n",
    "    F: The total forcing units required (should be within [0,1000])\n",
    "    \"\"\"\n",
    "    # This allows us to pass both 1D and 2D arrays of temperature\n",
    "    # Copying X to safely modify it later on (may not be necessary, but readable)\n",
    "    X_2d = np.atleast_2d(np.copy(X))\n",
    "\n",
    "    # Exclude days before the start of the growing season\n",
    "    X_2d = X_2d[:, int(t1) :]\n",
    "\n",
    "    # Exclude days with temperature below threshold\n",
    "    X_2d[X_2d < T] = 0\n",
    "\n",
    "    # Accumulate remaining data\n",
    "    S = np.cumsum(X_2d, axis=-1)\n",
    "\n",
    "    # Find first entry that exceeds the total forcing units required.\n",
    "    doy = np.argmax(S > F, axis=-1)\n",
    "\n",
    "    # Add t1 back to the result\n",
    "    return doy + t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use this model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 degrees every day:\n",
    "X_test = np.ones(365) * 10\n",
    "\n",
    "# Predicted spring onset:\n",
    "thermaltime(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 50, 50, 50, 50, 50, 50, 50, 50, 50])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also check for 2D X inputs:\n",
    "X_test = np.ones((10, 365)) * 10\n",
    "thermaltime(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it seems this works nicely, both for indivual prediction and for 2D arrays of inputs.\n",
    "\n",
    "## Adding tests\n",
    "\n",
    "These quick checks are super useful! We can quickly add a few more and add them to our test file (`test_thermaltime.py`). Note: also copy the `thermaltime` function from above to your file `thermaltime.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy these tests to your file thermaltime.py, uncomment the imports, and remove the bottom part.\n",
    "\n",
    "# Note: these imports must be uncommented in your test file\n",
    "# import numpy as np\n",
    "# from thermaltime import ThermalTime, thermaltime\n",
    "\n",
    "\n",
    "def test_1d_base_case():\n",
    "    # 10 degrees every day:\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert thermaltime(X_test) == 50\n",
    "\n",
    "\n",
    "def test_late_growing_season():\n",
    "    # If the growing season starts later, the spring onset is later as well.\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert thermaltime(X_test, t1=10) == 60\n",
    "\n",
    "\n",
    "def test_higher_threshold():\n",
    "    # If the total accumulated forcing required is higher, spring onset is later.\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert thermaltime(X_test, F=600) == 60\n",
    "\n",
    "\n",
    "def test_exclude_cold_days():\n",
    "    # If some days are below the minimum growing T, spring onset is later.\n",
    "    X_test = np.ones(365) * 10\n",
    "    X_test[[1, 4, 8, 12, 17, 24, 29, 33, 38, 42]] = 3\n",
    "    assert thermaltime(X_test) == 60\n",
    "\n",
    "\n",
    "def test_lower_temperature_threshold():\n",
    "    # If the minimum growing T is lower, fewer days are exluded. However, the\n",
    "    # accumulated temperature rises more slowly.\n",
    "    X_test = np.ones(365) * 10\n",
    "\n",
    "    X_test[[1, 4, 8, 12, 17, 24, 29, 33, 38, 42]] = 5\n",
    "    assert thermaltime(X_test, T=2) == 55\n",
    "\n",
    "\n",
    "def test_2d():\n",
    "    # Should be able to predict for multiple samples at once\n",
    "    X_test = np.ones((10, 365)) * 10\n",
    "    expected = np.ones(10) * 50\n",
    "    result = thermaltime(X_test)\n",
    "    assert np.all(result == expected)\n",
    "\n",
    "\n",
    "# Note: The following lines are not needed in your test file. Pytest will\n",
    "# automatically call all functions starting with \"test_\".\n",
    "test_1d_base_case()\n",
    "test_late_growing_season()\n",
    "test_higher_threshold()\n",
    "test_exclude_cold_days()\n",
    "test_lower_temperature_threshold()\n",
    "test_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've copied the code to your files, you can run pytest again to check that all new tests pass.\n",
    "\n",
    "Now that we're confident our new predict function works, the last thing we need to do is update the predict method on the class. Change it to look like this:\n",
    "\n",
    "```py\n",
    "    def predict(self, X: ArrayLike):\n",
    "        \"\"\"Predict values of y given new predictors\n",
    "\n",
    "        Parameters:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "               Daily mean temperatures for each unique site/year (n_samples) and\n",
    "               for each DOY (n_features). The first feature should correspond to\n",
    "               the first DOY, and so forth up to (max) 366.\n",
    "\n",
    "        Returns:\n",
    "            y: array-like, shape (n_samples,)\n",
    "               Predicted DOY of the spring onset for each sample in X.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X)\n",
    "        check_is_fitted(self, [\"t1_\", \"T_\", \"F_\"])\n",
    "\n",
    "        return thermaltime(X, self.t1_, self.T_, self.F_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the `fit` method\n",
    "\n",
    "Now that we can make predictions, we can also think about optimizing the parameters of the model. Our aim is to minimize the difference between the predictions based on the training data `X` and the target data `y`. Let's first prepare some training data to test the method once we have it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.74260126249088 43.93986197940313\n",
      "44.559769673387 57.8558220233387\n"
     ]
    }
   ],
   "source": [
    "# Prepare some training data. Take a base temperature of 10 degrees and add a\n",
    "# random fluctuation on top of it. The corresponding spring onset should\n",
    "# correlate with the random temperature fluctations.\n",
    "\n",
    "temp_signal = np.random.randn(10, 365)\n",
    "X_train = np.ones((10, 365)) * 10 + temp_signal * 10\n",
    "y_train = np.ones(10) * 50 + temp_signal.mean(axis=1) * 100\n",
    "\n",
    "# Check that the values are within somewhat realistic ranges\n",
    "print(X_train.min(), X_train.max())\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try scipy's curve-fit. It looks to have the exact signature we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.54942573,   5.        , 500.        ]),\n",
       " array([[10.81891166,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "initial_guess = [0, 5, 500]\n",
    "lower_bounds = [-67, -25, 0]\n",
    "upper_bounds = [298, 25, 1000]\n",
    "\n",
    "curve_fit(\n",
    "    thermaltime, X_train, y_train, p0=initial_guess, bounds=(lower_bounds, upper_bounds)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this gives terrible fits. Also: it doesn't like integer\n",
    "parameters (see https://stackoverflow.com/a/22861933).\n",
    "\n",
    "Probably this is what the developers of pyphenology also found. They are using a slighly different approach, with scipy's global optimizers instead. Let's try that, then.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check out the project template\n",
    "\n",
    "https://github.com/scikit-learn-contrib/project-template/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "springtime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
