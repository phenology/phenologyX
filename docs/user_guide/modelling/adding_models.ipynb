{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a (physics-based) model\n",
    "\n",
    "So far we've only looked at machine learning models. We are very keen to know\n",
    "how these \"smart\" approaches compare to more traditional, physics-based models.\n",
    "\n",
    "[PyPhenology](https://github.com/sdtaylor/pyPhenology) is a nice python package\n",
    "with a collection of physics-based models. We would like to compare those\n",
    "models, ideally within the same pycaret framework. However, pyPhenology is not\n",
    "consistent with the scikit-learn API. On the other hand, it is quite possible to\n",
    "cast their equations to a form that does adhere to these standards.\n",
    "\n",
    "In this notebook, we will walk you through the steps to create a custom\n",
    "estimator, following the [scikit-learn\n",
    "documentation](https://scikit-learn.org/stable/developers/develop.html). We will\n",
    "show how this is done for [pyPhenology's ThermalTime\n",
    "model](https://pyphenology.readthedocs.io/en/master/generated/pyPhenology.models.ThermalTime.html#pyPhenology.models.ThermalTime).\n",
    "At the end of the chapter, you should be able to repeat the trick for the other\n",
    "pyPhenology models as well.\n",
    "\n",
    "## The first blow is half the battle\n",
    "\n",
    "As a starting point, we copied the [scikit-learn project template](https://github.com/scikit-learn-contrib/project-template/blob/a06bc1a701fbb320848e4d5295e4477b596078df/skltemplate/_template.py) and updated it with some of the information from the [pyphenology ThermalTime](https://github.com/sdtaylor/pyPhenology/blob/d82af2f669364e84be4bf9325a4f4e064d8d3816/pyPhenology/models/thermaltime.py) class. Specifically, we:\n",
    "\n",
    "- Added\n",
    "  [RegressorMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html#sklearn.base.RegressorMixin)\n",
    "  from scikit-learn. This contains some methods specific to regression estimators.\n",
    "- Replaced `check_X_y` and `check_array` with the newer `_validate_data()` (see\n",
    "  [SLEP010](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html)).\n",
    "- Merged docstrings of ThermalTime and sklearn template\n",
    "- Changed to google-style docstrings and added type hints to method signatures instead of in the docstrings\n",
    "- Set the default values of the parameters to sensible ints instead of valid ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    RegressorMixin,\n",
    "    check_is_fitted,\n",
    ")\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "\n",
    "class ThermalTime(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"Thermal Time Model\n",
    "\n",
    "    The classic growing degree day model using a fixed temperature threshold\n",
    "    above which forcing accumulates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # TODO: consider adding DOY index series to fit/predict as optional argument\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike):\n",
    "        \"\"\"Fit the model to the available observations.\n",
    "\n",
    "        Parameters:\n",
    "            X: 2D Array of shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and\n",
    "                for each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            y: 1D Array of length n_samples\n",
    "                Observed DOY of the spring onset for each unique site/year.\n",
    "\n",
    "        Returns:\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        X, y = self._validate_data(X, y)\n",
    "        # TODO: check additional assumptions about input\n",
    "\n",
    "        # TODO: convert to proper fit; for now set some default values\n",
    "        self.t1_: int = 0\n",
    "        self.T_: int = 5\n",
    "        self.F_: int = 500\n",
    "\n",
    "        # `fit` should always return `self`\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: ArrayLike):\n",
    "        \"\"\"Predict values of y given new predictors\n",
    "\n",
    "        Parameters:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "               Daily mean temperatures for each unique site/year (n_samples) and\n",
    "               for each DOY (n_features). The first feature should correspond to\n",
    "               the first DOY, and so forth up to (max) 366.\n",
    "\n",
    "        Returns:\n",
    "            y: array-like, shape (n_samples,)\n",
    "               Predicted DOY of the spring onset for each sample in X.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X)\n",
    "        check_is_fitted(self, [\"t1_\", \"T_\", \"F_\"])\n",
    "\n",
    "        # TODO: Implement real predictions\n",
    "        return np.ones(X.shape[0], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! That's a big start! Notice that we're not passing in anything during initialization (yet). By convention of scikit-learn, the parameters of the model are only set during fit. Fitted parameters can be recognized by their trailing underscore.\n",
    "\n",
    "This class can already be used, although it doesn't\n",
    "actually fit or predict anything (useful) yet. Next, we need to\n",
    "\n",
    "- Provide an implementation for the predict method\n",
    "- Provide an implementation for the fit method\n",
    "- Consider doing more validation of the input, since now we simply assume that\n",
    "  the data is in ordere columns from DOY 1 up to (max) 366.\n",
    "- Consider allowing an additional argument to fit and predict that contains the\n",
    "  column indices in case they're not neatly formatted from 1 to (max) 366. This\n",
    "  is allowed by scikit-learn as long as it's an optional argument.\n",
    "\n",
    "However, before we proceed, let's see whether we already adhere to the\n",
    "scikit-learn API.\n",
    "\n",
    "## Checking sklearn compliance\n",
    "\n",
    "Scikit-learn provide a nice compliance checker. With a bit of extra code we can\n",
    "print out which tests it fails (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check check_regressors_train failed with exception: \n",
      "Check check_regressors_train failed with exception: \n",
      "Check check_regressors_train failed with exception: \n",
      "Passed checks: 37, failed checks: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "# This bit of code allows us to run the checks in a notebook\n",
    "checks = check_estimator(ThermalTime(), generate_only=True)\n",
    "passed_checks = 0\n",
    "failed_checks = 0\n",
    "for estimator, check in checks:\n",
    "    name = check.func.__name__\n",
    "    try:\n",
    "        check(estimator)\n",
    "        passed_checks += 1\n",
    "    except Exception as exc:\n",
    "        print(f\"Check {name} failed with exception: {exc}\")\n",
    "        failed_checks += 1\n",
    "print(f\"Passed checks: {passed_checks}, failed checks: {failed_checks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. Most of the checks passed, and if we dive deep into what's\n",
    "being check, we can figure out that the others failed because the predictions\n",
    "were not that good. That makes sense...\n",
    "\n",
    "## Using pytest and source files\n",
    "\n",
    "While it is possible to do all this in a notebook, a neater and more convenient\n",
    "way is to use pytest. To this end:\n",
    "\n",
    "- Store the class definition above in a new file called `thermaltime.py`\n",
    "- Create a new file called `test_thermaltime.py` and add the following content\n",
    "\n",
    "  ```py\n",
    "  from thermaltime import ThermalTime\n",
    "\n",
    "  from sklearn.utils.estimator_checks import parametrize_with_checks\n",
    "\n",
    "  @parametrize_with_checks([ThermalTime(),])\n",
    "  def test_sklearn_compatible_estimator(estimator, check):\n",
    "      check(estimator)\n",
    "  ```\n",
    "\n",
    "- Install pytest: `pip install pytest`\n",
    "- Run pytest: `pytest test_thermaltime.py`\n",
    "\n",
    "## Implementing predict\n",
    "\n",
    "We'll start by implementing the predict method. This is relatively\n",
    "straightforward. We'll write a simple function that takes both X and the\n",
    "parameters, and returns the expected DOY. For ease of reference, we copied the\n",
    "docstrings from above. This implementation should be exactly the same as in\n",
    "pyPhenology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy this function to your file thermaltime.py\n",
    "\n",
    "\n",
    "def thermaltime(X, t1: int = 1, T: int = 5, F: int = 500):\n",
    "    \"\"\"Make prediction with the thermaltime model.\n",
    "\n",
    "    X: array-like, shape (n_samples, n_features).\n",
    "       Daily mean temperatures for each unique site/year (n_samples) and for\n",
    "       each DOY (n_features). The first feature should correspond to\n",
    "       the first DOY, and so forth up to (max) 366.\n",
    "    t1: The DOY at which forcing accumulating beings (should be within [-67,298])\n",
    "    T: The threshold above which forcing accumulates (should be within [-25,25])\n",
    "    F: The total forcing units required (should be within [0,1000])\n",
    "    \"\"\"\n",
    "    # This allows us to pass both 1D and 2D arrays of temperature\n",
    "    # Copying X to safely modify it later on (may not be necessary, but readable)\n",
    "    X_2d = np.atleast_2d(np.copy(X))\n",
    "\n",
    "    # DOY starts at 1, where python array index start at 0\n",
    "    # TODO: Make this an optional argument?\n",
    "    doy = np.arange(X_2d.shape[1]) + 1\n",
    "\n",
    "    # Exclude days before the start of the growing season\n",
    "    X_2d[:, doy < t1] = 0\n",
    "\n",
    "    # Exclude days with temperature below threshold\n",
    "    X_2d[X_2d < T] = 0\n",
    "\n",
    "    # Accumulate remaining data\n",
    "    S = np.cumsum(X_2d, axis=-1)\n",
    "\n",
    "    # Find first entry that exceeds the total forcing units required.\n",
    "    doy = np.argmax(S > F, axis=-1)\n",
    "\n",
    "    return doy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use this model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 degrees every day:\n",
    "X_test = np.ones(365) * 10\n",
    "\n",
    "# Predicted spring onset:\n",
    "thermaltime(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 50, 50, 50, 50, 50, 50, 50, 50, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also check for 2D X inputs:\n",
    "X_test = np.ones((10, 365)) * 10\n",
    "thermaltime(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it seems this works nicely, both for indivual prediction and for 2D arrays of inputs.\n",
    "\n",
    "### Adding tests\n",
    "\n",
    "These quick checks are super useful! We can quickly add a few more and add them to our test file (`test_thermaltime.py`). Note: also copy the `thermaltime` function from above to your file `thermaltime.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Copy these tests to your file thermaltime.py, uncomment the imports, and remove the bottom part.\n",
    "\n",
    "# Note: these imports must be uncommented in your test file\n",
    "# import numpy as np\n",
    "# from thermaltime import ThermalTime, thermaltime\n",
    "\n",
    "\n",
    "def test_1d_base_case():\n",
    "    # 10 degrees every day:\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert thermaltime(X_test) == 50\n",
    "\n",
    "\n",
    "def test_late_growing_season():\n",
    "    # If the growing season starts later, the spring onset is later as well.\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert thermaltime(X_test, t1=11) == 60\n",
    "\n",
    "\n",
    "def test_higher_threshold():\n",
    "    # If the total accumulated forcing required is higher, spring onset is later.\n",
    "    X_test = np.ones(365) * 10\n",
    "    assert thermaltime(X_test, F=600) == 60\n",
    "\n",
    "\n",
    "def test_exclude_cold_days():\n",
    "    # If some days are below the minimum growing T, spring onset is later.\n",
    "    X_test = np.ones(365) * 10\n",
    "    X_test[[1, 4, 8, 12, 17, 24, 29, 33, 38, 42]] = 3\n",
    "    assert thermaltime(X_test) == 60\n",
    "\n",
    "\n",
    "def test_lower_temperature_threshold():\n",
    "    # If the minimum growing T is lower, fewer days are exluded. However, the\n",
    "    # accumulated temperature rises more slowly.\n",
    "    X_test = np.ones(365) * 10\n",
    "\n",
    "    X_test[[1, 4, 8, 12, 17, 24, 29, 33, 38, 42]] = 5\n",
    "    assert thermaltime(X_test, T=2) == 55\n",
    "\n",
    "\n",
    "def test_2d():\n",
    "    # Should be able to predict for multiple samples at once\n",
    "    X_test = np.ones((10, 365)) * 10\n",
    "    expected = np.ones(10) * 50\n",
    "    result = thermaltime(X_test)\n",
    "    assert np.all(result == expected)\n",
    "\n",
    "\n",
    "# Note: The following lines are not needed in your test file. Pytest will\n",
    "# automatically call all functions starting with \"test_\".\n",
    "test_1d_base_case()\n",
    "test_late_growing_season()\n",
    "test_higher_threshold()\n",
    "test_exclude_cold_days()\n",
    "test_lower_temperature_threshold()\n",
    "test_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've copied the code to your files, you can run pytest again to check that all new tests pass.\n",
    "\n",
    "Now that we're confident our new predict function works, the last thing we need to do is update the predict method on the class. Change it to look like this:\n",
    "\n",
    "```py\n",
    "    def predict(self, X: ArrayLike):\n",
    "        \"\"\"Predict values of y given new predictors\n",
    "\n",
    "        Parameters:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "               Daily mean temperatures for each unique site/year (n_samples) and\n",
    "               for each DOY (n_features). The first feature should correspond to\n",
    "               the first DOY, and so forth up to (max) 366.\n",
    "\n",
    "        Returns:\n",
    "            y: array-like, shape (n_samples,)\n",
    "               Predicted DOY of the spring onset for each sample in X.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X)\n",
    "        check_is_fitted(self, [\"t1_\", \"T_\", \"F_\"])\n",
    "\n",
    "        return thermaltime(X, self.t1_, self.T_, self.F_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the `fit` method\n",
    "\n",
    "Now that we can make predictions, we can also think about optimizing the parameters of the model.\n",
    "\n",
    "### Generate training data\n",
    "\n",
    "Our aim is to minimize the difference between the predictions based on the training data `X` and the target data `y`. Let's first prepare some training data to test the method once we have it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27.44049847799723 46.040299933994405\n",
      "40.856081333695 58.4199093709297\n"
     ]
    }
   ],
   "source": [
    "# Prepare some training data. Take a base temperature of 10 degrees and add a\n",
    "# random fluctuation on top of it. The corresponding spring onset should\n",
    "# correlate with the random temperature fluctations.\n",
    "\n",
    "temp_signal = np.random.randn(10, 365)\n",
    "X_train = np.ones((10, 365)) * 10 + temp_signal * 10\n",
    "y_train = np.ones(10) * 50 + temp_signal.mean(axis=1) * 100\n",
    "\n",
    "# Check that the values are within somewhat realistic ranges\n",
    "print(X_train.min(), X_train.max())\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring algorithms\n",
    "\n",
    "Scipy has a lot of [optimization routines](https://docs.scipy.org/doc/scipy/reference/optimize.html). Looking at the list of optimizers, it seems `curve-fit` has the exact signature we are looking for. However, if we try it, we will quickly find out that it is not fit for purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.00000005,   5.        , 500.        ]),\n",
       " array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "initial_guess = [1, 5, 500]\n",
    "lower_bounds = [-67, -25, 0]\n",
    "upper_bounds = [298, 25, 1000]\n",
    "\n",
    "curve_fit(\n",
    "    thermaltime, X_train, y_train, p0=initial_guess, bounds=(lower_bounds, upper_bounds)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives terrible fits! Or rather, it doesn't seem to do much fitting at all. If you play around with the initial settings you'll see that most of the output is rubbish. That's probably because our prediction model is not a nice, continuous functional form. Also, `curve-fit` [doesn't really like](https://stackoverflow.com/a/22861933) integer parameters such as the DOY (though our implementation doesn't really care either).\n",
    "\n",
    "Clearly, we need to look further.\n",
    "\n",
    "### Reformulating the problem\n",
    "\n",
    "Alternatively, following pyphenology, we can try the [global optimizers](https://docs.scipy.org/doc/scipy/reference/optimize.html) instead. Looking at the documentation of the [brute-force optimizer](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brute.html#scipy.optimize.brute), it seems we need to reformulate our problem a little bit. First, we need to define a loss function, i.e. the function that scipy can minimize.\n",
    "\n",
    "Intuitively, our loss function could look something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.2831808222873"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse_loss(X, y, t1: int = 1, T: int = 5, F: int = 500):\n",
    "    y_pred = thermaltime(X, t1=t1, T=T, F=F)\n",
    "    sq_err = (y_pred - y) ** 2\n",
    "    return np.mean(sq_err) ** 0.5\n",
    "\n",
    "\n",
    "# Try it with the default parameters:\n",
    "rmse_loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not consistent with the form expected by scipy's global optimizers: they want something of the form `rmse_loss(x, *args)`. So we need to reformulate our loss function in terms of `x` and `args`.\n",
    "\n",
    "Nota bene: where the brute-force example finds the minimum of the function in the x-y space given fixed parameters, we want to do the opposite: we want to find the minimum in the parameter space, given the (fixed for each training batch) X and y. So, `x` is a tuple of our models params, `*args` corresponds to `X` and `y`, and our loss function becomes something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.2831808222873"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse_loss(params, X, y):\n",
    "    \"\"\"RMSE loss between thermaltime predictions and observations.\n",
    "\n",
    "    params is a tuple with the parameters to thermaltime: (t1, T, F)\n",
    "    \"\"\"\n",
    "    y_pred = thermaltime(X, *params)\n",
    "    sq_err = (y_pred - y) ** 2\n",
    "    return np.mean(sq_err) ** 0.5\n",
    "\n",
    "\n",
    "# Try it with the default parameters:\n",
    "rmse_loss([1, 5, 500], X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above is good enough, we can stil tweak our loss function a bit further while we're at it. Currently, our loss has the `thermaltime` function hardcoded. It would be much nicer if this was more flexible, so it could work for any (roughly similar) model, for example, all the pyphenology models.\n",
    "\n",
    "One way to do this is to pass the predictor function as one of the arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.2831808222873"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse_loss(params, f, X, y):\n",
    "    \"\"\"RMSE loss between thermaltime predictions and observations.\n",
    "\n",
    "    f is a prediction model of the form f(X, *params)\n",
    "    params is a tuple with the parameters to f\n",
    "    \"\"\"\n",
    "    y_pred = f(X, *params)\n",
    "    sq_err = (y_pred - y) ** 2\n",
    "    return np.mean(sq_err) ** 0.5\n",
    "\n",
    "\n",
    "# Try it with the default parameters:\n",
    "rmse_loss([1, 5, 500], thermaltime, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force optimization\n",
    "\n",
    "Now we're ready to run the brute force optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.67631579, 14.47368421,  0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import brute\n",
    "\n",
    "ranges = ((-67, 298), (-25, 25), (0, 1000))\n",
    "args = (thermaltime, X_train, y_train)\n",
    "\n",
    "brute(rmse_loss, ranges=ranges, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works! If you look carefully at the results, you will see that the optimization algorithm finds a clever solution to the problem: it sets the total forcing required to 0, and the first day of the growing season very close to the typical spring onset. While this is a perfectly valid solution, especially considering the fake training data we used, it illustrates that you need to be really careful in setting the expected bounds of the parameters, and in interpreting the results.\n",
    "\n",
    "Another thing to note is how scipy creates the search space. The `brute` function has an additional parameter `Ns` that determines the number of 'grid points' used for each of the parameters. The default is 20: the ranges we supplied for `t1`, `T`, and `F` are divided in 20 points, such that the algorithm executes our loss function `20 * 20 * 20` times. By increasing the number of points, we get a better estimate, but the number of function calls and therefore the training time increases exponentially.\n",
    "\n",
    "There are two ways around this:\n",
    "\n",
    "- Set the points yourself\n",
    "- Use a more clever algorithm\n",
    "\n",
    "The first solution is quite simple. We can also define the ranges as slices with a custom `step`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 53., -25.,   0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranges = (slice(-67, 298, 10), slice(-25, 25, 2), slice(0, 1000, 50))\n",
    "\n",
    "brute(rmse_loss, ranges=ranges, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basin hopping\n",
    "\n",
    "For the second solution, again, we follow the pyphenology developers. They support \"differential evolution\" and \"basin_hopping\". Let's try the basin hopping algorithm. This algorithm works in two steps, delegating calls to our loss function to a local minimization function. Thus, we need to pass in our bounds and args as `minimzer_kwargs` instead. We must also pick a method for the local minizer. For consistency with pyphenology, we choose 'LL-BFGS-B'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    message: ['requested number of basinhopping iterations completed successfully']\n",
       "                    success: True\n",
       "                        fun: 10.389960273765986\n",
       "                          x: [ 2.244e+00  5.994e+00  4.984e+02]\n",
       "                        nit: 100\n",
       "      minimization_failures: 0\n",
       "                       nfev: 472\n",
       "                       njev: 118\n",
       " lowest_optimization_result:  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "                              success: True\n",
       "                               status: 0\n",
       "                                  fun: 10.389960273765986\n",
       "                                    x: [ 2.244e+00  5.994e+00  4.984e+02]\n",
       "                                  nit: 0\n",
       "                                  jac: [ 0.000e+00  0.000e+00  0.000e+00]\n",
       "                                 nfev: 4\n",
       "                                 njev: 1\n",
       "                             hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import basinhopping\n",
    "\n",
    "initial_guess = (1, 5, 500)\n",
    "minimizer_args = {\n",
    "    \"method\": \"L-BFGS-B\",\n",
    "    \"args\": (thermaltime, X_train, y_train),\n",
    "    \"bounds\": ((-67, 298), (-25, 25), (0, 1000)),\n",
    "}\n",
    "basinhopping(rmse_loss, x0=initial_guess, minimizer_kwargs=minimizer_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also works, but notice that the result is still very close to the initial\n",
    "guess. If we change the initial guess to something that's more like the clever\n",
    "solution of the brute method, we find another fit much smaller RMSE value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [47.04348765 15.83528259  2.65394513], minimum: 6.633890538987595\n"
     ]
    }
   ],
   "source": [
    "initial_guess = (45, 15, 0)\n",
    "minimizer_args = {\n",
    "    \"method\": \"L-BFGS-B\",\n",
    "    \"args\": (thermaltime, X_train, y_train),\n",
    "    \"bounds\": ((-67, 298), (-25, 25), (0, 1000)),\n",
    "}\n",
    "\n",
    "bh = basinhopping(rmse_loss, x0=initial_guess, minimizer_kwargs=minimizer_args)\n",
    "print(f\"params: {bh.x}, minimum: {bh.fun}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can pass additional arguments to the basinhopping algorithm to tweak its performance. For example, the default of 100 iterations is quite small. Using the settings from pyphenology, we get a result that looks much more realistic (and takes much longer to train):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    message: ['requested number of basinhopping iterations completed successfully']\n",
       "                    success: True\n",
       "                        fun: 5.3850952685237115\n",
       "                          x: [ 5.078e+01 -7.646e+00  9.683e+00]\n",
       "                        nit: 50000\n",
       "      minimization_failures: 0\n",
       "                       nfev: 200072\n",
       "                       njev: 50018\n",
       " lowest_optimization_result:  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
       "                              success: True\n",
       "                               status: 0\n",
       "                                  fun: 5.3850952685237115\n",
       "                                    x: [ 5.078e+01 -7.646e+00  9.683e+00]\n",
       "                                  nit: 0\n",
       "                                  jac: [ 0.000e+00  0.000e+00  0.000e+00]\n",
       "                                 nfev: 4\n",
       "                                 njev: 1\n",
       "                             hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import basinhopping\n",
    "\n",
    "initial_guess = (1, 5, 500)\n",
    "minimizer_args = {\n",
    "    \"method\": \"L-BFGS-B\",\n",
    "    \"args\": (thermaltime, X_train, y_train),\n",
    "    \"bounds\": ((-67, 298), (-25, 25), (0, 1000)),\n",
    "}\n",
    "optimizer_params = {\"niter\": 50000, \"T\": 0.5, \"stepsize\": 0.5, \"disp\": False}\n",
    "basinhopping(\n",
    "    rmse_loss, x0=initial_guess, **optimizer_params, minimizer_kwargs=minimizer_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a good grip on the different ways to fit our model. A final thing to point out is that the basinhopping algorithm returns floats for each parameter. This doesn't really make sense for the start of the growing season, so in the post-processing we should probably round that to the nearest int.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing everything together\n",
    "\n",
    "If we bring everything from above over to the `fit` method, we get something like this:\n",
    "\n",
    "```python\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike):\n",
    "        \"\"\"Fit the model to the available observations.\n",
    "\n",
    "        Parameters:\n",
    "            X: 2D Array of shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and\n",
    "                for each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            y: 1D Array of length n_samples\n",
    "                Observed DOY of the spring onset for each unique site/year.\n",
    "\n",
    "        Returns:\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        X, y = self._validate_data(X, y)\n",
    "        # TODO: check additional assumptions about input\n",
    "\n",
    "        initial_guess = (1, 5, 500)\n",
    "        bounds = ((-67, 298), (-25, 25), (0, 1000))\n",
    "\n",
    "        minimizer_args = {\n",
    "            \"method\": \"L-BFGS-B\",\n",
    "            \"args\": (thermaltime, X, y),\n",
    "            \"bounds\": bounds,\n",
    "        }\n",
    "        optimizer_params = {\"niter\": 50000, \"T\": 0.5, \"stepsize\": 0.5, \"disp\": False}\n",
    "\n",
    "        bh = basinhopping(\n",
    "            partial(rmse_loss, f=thermaltime, X=X, y=y),\n",
    "            x0=initial_guess,\n",
    "            minimizer_kwargs=minimizer_args,\n",
    "            **optimizer_params,\n",
    "        )\n",
    "\n",
    "        self.t1_, self.T_, self.F_ = bh.x\n",
    "\n",
    "        return self\n",
    "```\n",
    "\n",
    "This is not yet ready:\n",
    "\n",
    "- `Initial guess` and `bounds` are hardcoded and closely related to the equally hardcoded `thermaltime` and the loss function. If we want to reuse this for other physics-based models, it would be nice to try and separate these a bit better.\n",
    "- The `optimizer_params` are hardcoded as well. These are typical (hyper) parameters that can be passed to the `__init__` method of our class, since these settings are not data-dependent.\n",
    "- Currently the only optimization routine supported is the basinhopping algorithm.\n",
    "\n",
    "The version below is slightly better. We generalized the loss function a bit better, and the core_model-related parameters are grouped together at the top. The fit method is mostly concerned with formulating the call to the basinhopping algorithm. We used the `staticmethod` decorator to make the thermaltime function a method on the class. Notice that, in contrast with standard class methods, it doesn't need access to `self`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    RegressorMixin,\n",
    "    check_is_fitted,\n",
    ")\n",
    "from numpy.typing import ArrayLike\n",
    "from scipy.optimize import basinhopping\n",
    "\n",
    "\n",
    "def rmse_loss(params, f, X, y):\n",
    "    \"\"\"RMSE loss between thermaltime predictions and observations.\n",
    "\n",
    "    f is a prediction model of the form f(X, *params)\n",
    "    params is a tuple with the parameters to f\n",
    "    \"\"\"\n",
    "    y_pred = f(X, *params)\n",
    "    sq_err = (y_pred - y) ** 2\n",
    "    return np.mean(sq_err) ** 0.5\n",
    "\n",
    "\n",
    "LOSS_FUNCTIONS = {\n",
    "    \"RMSE\": rmse_loss,\n",
    "}\n",
    "\n",
    "\n",
    "class ThermalTime(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"Thermal Time Model\n",
    "\n",
    "    The classic growing degree day model using a fixed temperature threshold\n",
    "    above which forcing accumulates.\n",
    "\n",
    "    This implementation uses scipy's basinhopper optimization algorithm for\n",
    "    fitting the model.\n",
    "    \"\"\"\n",
    "\n",
    "    _core_params_names = (\"t1\", \"T\", \"F\")\n",
    "    _core_params_defaults = (1, 5, 500)\n",
    "    _core_params_bounds = ((-67, 298), (-25, 25), (0, 1000))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        niter=50000,\n",
    "        T=0.5,\n",
    "        stepsize=0.5,\n",
    "        disp=False,\n",
    "        minimizer_method=\"L-BFGS-B\",\n",
    "        loss_function=\"RMSE\",\n",
    "    ):\n",
    "        self.niter = niter\n",
    "        self.T = T\n",
    "        self.stepsize = stepsize\n",
    "        self.disp = disp\n",
    "        self.minimizer_method = minimizer_method\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    @staticmethod\n",
    "    def _core_model(X, t1: int = 1, T: int = 5, F: int = 500):\n",
    "        \"\"\"Make prediction with the thermaltime model.\n",
    "\n",
    "        Args:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and for\n",
    "                each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            t1: The DOY at which forcing accumulating beings (should be within [-67,298])\n",
    "            T: The threshold above which forcing accumulates (should be within [-25,25])\n",
    "            F: The total forcing units required (should be within [0,1000])\n",
    "        \"\"\"\n",
    "        # This allows us to pass both 1D and 2D arrays of temperature\n",
    "        # Copying X to safely modify it later on (may not be necessary, but readable)\n",
    "        X_2d = np.atleast_2d(np.copy(X))\n",
    "\n",
    "        # DOY starts at 1, where python array index start at 0\n",
    "        # TODO: Make this an optional argument?\n",
    "        doy = np.arange(X_2d.shape[1]) + 1\n",
    "\n",
    "        # Exclude days before the start of the growing season\n",
    "        X_2d[:, doy < t1] = 0\n",
    "\n",
    "        # Exclude days with temperature below threshold\n",
    "        X_2d[X_2d < T] = 0\n",
    "\n",
    "        # Accumulate remaining data\n",
    "        S = np.cumsum(X_2d, axis=-1)\n",
    "\n",
    "        # Find first entry that exceeds the total forcing units required.\n",
    "        doy = np.argmax(S > F, axis=-1)\n",
    "\n",
    "        return doy\n",
    "\n",
    "    # TODO: consider adding DOY index series to fit/predict as optional argument\n",
    "    def fit(self, X: ArrayLike, y: ArrayLike):\n",
    "        \"\"\"Fit the model to the available observations.\n",
    "\n",
    "        Parameters:\n",
    "            X: 2D Array of shape (n_samples, n_features).\n",
    "                Daily mean temperatures for each unique site/year (n_samples) and\n",
    "                for each DOY (n_features). The first feature should correspond to\n",
    "                the first DOY, and so forth up to (max) 366.\n",
    "            y: 1D Array of length n_samples\n",
    "                Observed DOY of the spring onset for each unique site/year.\n",
    "\n",
    "        Returns:\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        X, y = self._validate_data(X, y)\n",
    "        # TODO: check additional assumptions about input\n",
    "\n",
    "        loss_function = LOSS_FUNCTIONS[self.loss_function]\n",
    "\n",
    "        # Perform the fit\n",
    "        bh = basinhopping(\n",
    "            loss_function,\n",
    "            x0=self._core_params_defaults,\n",
    "            niter=self.niter,\n",
    "            T=self.T,\n",
    "            stepsize=self.stepsize,\n",
    "            disp=self.disp,\n",
    "            minimizer_kwargs={\n",
    "                \"method\": self.minimizer_method,\n",
    "                \"args\": (self._core_model, X, y),\n",
    "                \"bounds\": self._core_params_bounds,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Store the fitted parameters\n",
    "        self.core_params_ = bh.x\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: ArrayLike):\n",
    "        \"\"\"Predict values of y given new predictors\n",
    "\n",
    "        Parameters:\n",
    "            X: array-like, shape (n_samples, n_features).\n",
    "               Daily mean temperatures for each unique site/year (n_samples) and\n",
    "               for each DOY (n_features). The first feature should correspond to\n",
    "               the first DOY, and so forth up to (max) 366.\n",
    "\n",
    "        Returns:\n",
    "            y: array-like, shape (n_samples,)\n",
    "               Predicted DOY of the spring onset for each sample in X.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X)\n",
    "        check_is_fitted(self, \"core_params_\")\n",
    "\n",
    "        return self._core_model(X, *self.core_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Update tests:\n",
    "\n",
    "* redefine thermaltime\n",
    "* set n_iter to something more sensible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "springtime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
