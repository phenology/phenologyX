{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example workflow\n",
    "Example \"recipe\" for downloading data from various sources and executing a range of ML methods with it. \n",
    "\n",
    "```yaml\n",
    "time:\n",
    "  default: 2000, 2001\n",
    "  ancient: 1980, 1990\n",
    "  modern: 2016, 2021\n",
    "\n",
    "geometry: \n",
    "  sites:\n",
    "    site_a: 40, -10\n",
    "    site_b: 30, 0\n",
    "    site_c: 20, 10\n",
    "    site_d: 10, 20\n",
    "\n",
    "  areas:\n",
    "    europe: -10, 40, 40, 70\n",
    "    usa: ...\n",
    "    name_x: ...\n",
    "\n",
    "datasets:\n",
    "  ppo:\n",
    "    engine: springtime  # or rppo, ...\n",
    "    period: default  # or e.g. [ancient, modern]\n",
    "    area: Europe  # or e.g. \"from_sites, radius=10, agg=mean\"\n",
    "    genus: Syringa\n",
    "    source: PEP725\n",
    "    termID: \"obo:PPO_0002313\"\n",
    "    filter:\n",
    "      minimum_availability: 80  # percent?\n",
    "  daymet:\n",
    "    engine: daymetr  # or REST api, ... (see https://daymet.ornl.gov/getdata)\n",
    "    variables: [tmin, tmax, sunshine_duration]\n",
    "  modis:\n",
    "    sites: all  # or [site_a, site_b], ... OR: from_dataset: ppo (will extract unique locations from result of ppo and download those)\n",
    "    variables: NDVI and EVI  # vegetation indices\n",
    "    engine: modistools\n",
    "    ...\n",
    "  pyphenology:\n",
    "    period: ...  # if not given, use first option from periods section above\n",
    "    species: vaccinium  # or aspen\n",
    "    phenophase: budburst  # or flowers\n",
    "\n",
    "cross_validation:\n",
    "  train_test_strategy: ShuffleSplit\n",
    "  metric_name: RMSE\n",
    "\n",
    "models:\n",
    "  target: day_of_first_bloom\n",
    "  sklearn: \n",
    "    model: sklearn.linear_model\n",
    "    options: ...\n",
    "  pyphenology:\n",
    "    model: pyPhenology.primary_model\n",
    "    options: ...\n",
    "  merf: ...  # https://github.com/manifoldai/merf\n",
    "  interpretml: ...  # https://github.com/interpretml/interpret/\n",
    "  statsmodels: ... # https://www.statsmodels.org/stable/mixed_linear.html \n",
    "  \n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import Point, Polygon\n",
    "from typing import Union, List, Tuple, Literal, Protocol\n",
    "from pathlib import Path\n",
    "from datetime import Date\n",
    "\n",
    "class Dataset(Protocol):\n",
    "    \"\"\"Interface for working with phenology datasets.\"\"\"\n",
    "\n",
    "    name: Literal[\"ppo\", \"daymet\", \"modis\", \"pyphenology\"]\n",
    "    period: Tuple[Date, Date]\n",
    "    geometry: Union[Point, Polygon]  # might need to convert bbox to polygon in parsing\n",
    "    \n",
    "\n",
    "    def get_files(self) -> List[Path]:\n",
    "        \"\"\"Show filename(s) that this dataset would have on disk.\n",
    "        \n",
    "        Should use a generic data reference sytax combined with a local\n",
    "        filesystem configuration. \n",
    "        \"\"\"\n",
    "\n",
    "    def exists_locally(self) -> bool:\n",
    "        \"\"\"Tell if the data is already present on disk.\"\"\"\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the data.\"\"\"\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load the dataset from disk into memory.\n",
    "        \n",
    "        This may include pre-processing operations as specified by the context, e.g. \n",
    "        filter certain variables, remove data points with too many NaNs, reshape data.\n",
    "        \"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Protocol):\n",
    "    \"\"\"Interface for working with various ML models.\"\"\"\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit model to data.\"\"\"\n",
    "\n",
    "    def predict(self, new_x):\n",
    "        \"\"\"Make a prediction for new data.\"\"\"\n",
    "        \n",
    "\n",
    "class CV(Protocol):\n",
    "    \"\"\"Interface for cross-validation strategy.\"\"\"\n",
    "    \n",
    "    def split(self, data):\n",
    "        \"\"\"Split the data into train/test sets.\"\"\"\n",
    "\n",
    "    def search(self):\n",
    "        \"\"\"Do grid search or something like that...\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "DEFAULT_CONFIG = \"~/.config/sprintime.yaml\"\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Settings for springtime.\"\"\"\n",
    "\n",
    "\n",
    "class Session():\n",
    "    \"\"\"Session for executing a workflow.\"\"\"\n",
    "    output_dir: Path\n",
    "\n",
    "\n",
    "class Workflow:\n",
    "\n",
    "    config: Config\n",
    "    recipe: Path\n",
    "    datasets: List[Dataset]\n",
    "    cross_validation: CV\n",
    "    models: List[Model]\n",
    "\n",
    "    @classmethod\n",
    "    def from_recipe(cls, recipe: Path):\n",
    "        with open(recipe, 'r') as raw_recipe:\n",
    "            options = yaml.load(raw_recipe)\n",
    "        \n",
    "        return cls(**options)\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"(Down)load data, pre-process, run models, evaluate.\"\"\"\n",
    "        self.create_session()\n",
    "        self.autocomplete()\n",
    "        self.download_data()\n",
    "        self.load_data()\n",
    "        self.run_experiments()\n",
    "\n",
    "    def create_session(self):\n",
    "        \"\"\"Create a context for executing the experiment.\"\"\"\n",
    "        self.session = Session(...)\n",
    "        self.recipe.copy(self.session.output_dir / \"data.csv\")\n",
    "        \n",
    "    def autocomplete(self):\n",
    "        \"\"\"Substitute time and area in datasets and model function mappings.\"\"\"\n",
    "\n",
    "    def download_data(self):\n",
    "        \"\"\"Download the data.\"\"\"\n",
    "        for dataset in self.datasets:\n",
    "            if not dataset.exists_locally() or self.config.force_override:\n",
    "                dataset.download()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load and merge input datasets.\"\"\"\n",
    "        data = []\n",
    "        for dataset in self.datasets:\n",
    "            \n",
    "            if not dataset.exists_locally():\n",
    "                dataset.download()\n",
    "            \n",
    "            data.append(dataset.load())\n",
    "\n",
    "        df = pd.concat(data, axis=1)        \n",
    "        df.to_csv(self.session.output_dir / \"data.csv\")\n",
    "        self.df = df\n",
    "\n",
    "    def run_experiments(self):\n",
    "        \"\"\"Train and evaluate ML models.\"\"\"\n",
    "        scores = {}\n",
    "        for model in self.models:\n",
    "            with self.cross_validation as cv:\n",
    "                model.fit(self.df)\n",
    "                score = model.score()\n",
    "                scores[model] = score\n",
    "\n",
    "        scores.df.to_csv(self.session.output_dir / \"data.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import sys\n",
    "    \n",
    "    recipe_file = sys.argv[0]\n",
    "    Workflow.from_recipe(recipe_file).execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "springtime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2fedef8e2270eddbf220d4503a289a780c93022acc0a45c760e2ab231247b99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
